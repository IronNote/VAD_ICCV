import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import numpy as np
import cv2
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation, FFMpegWriter
from sklearn.metrics import average_precision_score, roc_auc_score
from transformers import CLIPProcessor, CLIPModel
import os
import glob
import re
import ast

from model import CLIPVAD
from utils.dataset import UCFDataset
from utils.tools import get_batch_mask, get_prompt_text
from utils.ucf_detectionMAP import getDetectionMAP as dmAP
import ucf_option

# **********************************************************************************************************
# ğŸ“Œ ì›ë³¸ ë¹„ë””ì˜¤ ê²½ë¡œ ì°¾ê¸° (ì¹´í…Œê³ ë¦¬ ìë™ ì¶”ì¶œ)
# **********************************************************************************************************

def get_original_video_path(video_root_folder, base_name):
    """
    ì£¼ì–´ì§„ ë¹„ë””ì˜¤ íŒŒì¼ëª…ì—ì„œ ì¹´í…Œê³ ë¦¬ë¥¼ ì¶”ì¶œí•˜ê³ , í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œ ì›ë³¸ ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ê¸°
    """
    base_name_cleaned = re.sub(r"__\d+$", "", base_name)  # "__ìˆ«ì" ì œê±°
    category = base_name_cleaned.split("0")[0]  # ì²« ìˆ«ì ì´ì „ê¹Œì§€ê°€ ì¹´í…Œê³ ë¦¬

    video_folder = os.path.join(video_root_folder, category)
    video_candidates = glob.glob(os.path.join(video_folder, f"{base_name_cleaned}*"))

    if len(video_candidates) == 0:
        print(f"âŒ Error: No matching video found for {base_name_cleaned} in {video_folder}")
        return None, category
    elif len(video_candidates) > 1:
        print(f"âš  Warning: Multiple matching videos found for {base_name_cleaned}, selecting the first one.")

    video_path = video_candidates[0]
    print(f"âœ… Original video path: {video_path}")
    return video_path, category

# **********************************************************************************************************
# ğŸ“Œ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰ (AUC, AP, mAP í‰ê°€)
# **********************************************************************************************************

def test(model, testdataloader, maxlen, prompt_text, gt, gtsegments, gtlabels, device):
    """
    ğŸ“Œ test í•¨ìˆ˜: ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€
    - AUC, AP, mAPë¥¼ í†µí•´ anomaly detection ì„±ëŠ¥ ì¸¡ì •
    - testdataloaderì—ì„œ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì™€ ëª¨ë¸ ì¶”ë¡  í›„ ì„±ëŠ¥ ë¶„ì„
    """

    model.to(device)
    model.eval()

    element_logits2_stack = []

    with torch.no_grad():
        for i, item in enumerate(testdataloader):
            visual = item[0].squeeze(0)
            length = int(item[2])

            if length < maxlen:
                visual = visual.unsqueeze(0)
            
            visual = visual.to(device)

            # ğŸ“Š ë¹„ë””ì˜¤ ë°ì´í„°ë¥¼ segment ë‹¨ìœ„ë¡œ ë‚˜ëˆ”
            lengths = torch.zeros(int(length / maxlen) + 1, dtype=int)
            for j in range(len(lengths)):
                lengths[j] = min(maxlen, length)
                length -= maxlen
            
            padding_mask = get_batch_mask(lengths, maxlen).to(device)
            
            # ğŸ¯ ëª¨ë¸ ì¶”ë¡ 
            _, logits1, logits2 = model(visual, padding_mask, prompt_text, lengths)
            logits1 = logits1.reshape(-1, logits1.shape[2])
            logits2 = logits2.reshape(-1, logits2.shape[2])
            
            # ğŸŸ¢ ì´ìƒ í–‰ë™ í™•ë¥  ê³„ì‚°
            prob1 = torch.sigmoid(logits1[:length].squeeze(-1))
            prob2 = (1 - logits2[:length].softmax(dim=-1)[:, 0].squeeze(-1)) # í•´ë‹¹ ê²°ê³¼ê°€ ì–´ë–¤ ì˜ë¯¸? ì°¨ì›()

            if i == 0:
                ap1 = prob1
                ap2 = prob2
            else:
                ap1 = torch.cat([ap1, prob1], dim=0)
                ap2 = torch.cat([ap2, prob2], dim=0)

            element_logits2 = logits2[:length].softmax(dim=-1).cpu().numpy()
            element_logits2 = np.repeat(element_logits2, 16, axis=0)
            element_logits2_stack.append(element_logits2)

    # AUC, AP ì„±ëŠ¥ í‰ê°€
    ap1 = np.repeat(ap1.cpu().numpy(), 16)
    ap2 = np.repeat(ap2.cpu().numpy(), 16)

    ROC1 = roc_auc_score(gt, ap1)
    AP1 = average_precision_score(gt, ap1)
    ROC2 = roc_auc_score(gt, ap2)
    AP2 = average_precision_score(gt, ap2)

    print("ğŸ“Š AUC1:", ROC1, " AP1:", AP1)
    print("ğŸ“Š AUC2:", ROC2, " AP2:", AP2)

    # Mean Average Precision(mAP) í‰ê°€
    dmap, iou = dmAP(element_logits2_stack, gtsegments, gtlabels, excludeNormal=False)
    avg_map = np.mean(dmap)

    for i, val in enumerate(dmap):
        print(f"ğŸ“Š mAP@{iou[i]:.1f} = {val:.2f}%")
    print(f"ğŸ“Š í‰ê·  mAP: {avg_map:.2f}%\n")

    return ROC1, AP1

# **********************************************************************************************************
# ğŸ“Œ ì¹´í…Œê³ ë¦¬ë³„ í…ŒìŠ¤íŠ¸ ìˆ˜í–‰
# **********************************************************************************************************

def plot_category_performance(category_results, gt_dir):
    """
    ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ AUC, AP ì„±ëŠ¥ì„ ì‹œê°í™”í•˜ì—¬ ì €ì¥í•˜ëŠ” í•¨ìˆ˜
    """
    categories = list(category_results.keys())
    auc1_scores = [category_results[cat]["AUC1"] for cat in categories]
    ap1_scores = [category_results[cat]["AP1"] for cat in categories]
    auc2_scores = [category_results[cat]["AUC2"] for cat in categories]
    ap2_scores = [category_results[cat]["AP2"] for cat in categories]

    x = range(len(categories))

    plt.figure(figsize=(15, 6))
    bars1 = plt.bar(x, auc1_scores, width=0.2, label="AUC1", color="blue", alpha=0.7)
    plt.bar([i + 0.2 for i in x], ap1_scores, width=0.2, label="AP1", color="green", alpha=0.7)
    plt.bar([i + 0.4 for i in x], auc2_scores, width=0.2, label="AUC2", color="red", alpha=0.7)
    plt.bar([i + 0.6 for i in x], ap2_scores, width=0.2, label="AP2", color="orange", alpha=0.7)

    plt.xticks([i + 0.3 for i in x], categories, rotation=90)
    plt.ylabel("Score")
    plt.title("Category-wise Anomaly Detection Performance")
    plt.legend()

    # âœ… AUC1 ë§‰ëŒ€ ìœ„ì— ìˆ˜ì¹˜ í‘œì‹œ
    for bar in bars1:
        yval = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2, yval + 0.02, f"{yval:.2f}", ha="center", fontsize=10, fontweight="bold")

    save_path = os.path.join(gt_dir, "category_performance.png")
    plt.savefig(save_path, bbox_inches="tight", dpi=300)
    plt.close()
    print(f"ğŸ“‚ Saved performance plot to {save_path}")

def test_category_wise(model, gt_dir, maxlen, prompt_text, label_map, device):
    """
    ğŸ“Œ ì¹´í…Œê³ ë¦¬ë³„ anomaly detection ì„±ëŠ¥ í‰ê°€ (Frame-Level)
    """

    model.to(device)
    model.eval()

    category_results = {}

    category_gt_files = [f for f in os.listdir(gt_dir) if f.endswith("_GT.npy")]
    category_names = [f.replace("_GT.npy", "") for f in category_gt_files]

    for category in category_names:
        print(f"\nğŸš€ Evaluating {category} category...\n")

        category_test_list = os.path.join(gt_dir, f"{category}_category_test.csv")
        if not os.path.exists(category_test_list):
            print(f"âš  {category}_category_test.csv íŒŒì¼ì´ ì—†ìŒ - ê±´ë„ˆëœ€")
            continue

        testdataset = UCFDataset(maxlen, category_test_list, True, label_map)  
        testdataloader = DataLoader(testdataset, batch_size=1, shuffle=False)

        category_gt = np.load(os.path.join(gt_dir, f"{category}_GT.npy"))
        print(f"ğŸ“‚ Loaded GT for {category} (shape: {category_gt.shape})")

        ap1_list = []
        ap2_list = []

        with torch.no_grad():
            for i, item in enumerate(testdataloader):
                visual = item[0].squeeze(0)
                length = int(item[2])

                if length < maxlen:
                    visual = visual.unsqueeze(0)

                visual = visual.to(device)

                lengths = torch.zeros(int(length / maxlen) + 1, dtype=int)
                for j in range(len(lengths)):
                    lengths[j] = min(maxlen, length)
                    length -= maxlen

                padding_mask = get_batch_mask(lengths, maxlen).to(device)

                _, logits1, logits2 = model(visual, padding_mask, prompt_text, lengths)
                logits1 = logits1.reshape(-1, logits1.shape[2])
                logits2 = logits2.reshape(-1, logits2.shape[2])

                prob1 = torch.sigmoid(logits1[:length].squeeze(-1)).cpu().numpy()
                prob2 = (1 - logits2[:length].softmax(dim=-1)[:, 0].squeeze(-1)).cpu().numpy()
                
                ap1_list.append(prob1)
                ap2_list.append(prob2)

        ap1 = np.concatenate(ap1_list, axis=0)
        ap2 = np.concatenate(ap2_list, axis=0)
        ap1 = np.repeat(ap1, 16)
        ap2 = np.repeat(ap2, 16)

        print(f"\nğŸ“Œ [Category: {category}] GT vs AP í¬ê¸° ë¹„êµ")
        print(f"âœ… GT í¬ê¸°: {len(category_gt)}, AP1 í¬ê¸°: {len(ap1)}")

        unique_gt_values = np.unique(category_gt)
        print(f"ğŸ›  [DEBUG] {category} GT unique values: {unique_gt_values}")

        if len(unique_gt_values) < 2:
            print(f"âš  [ì£¼ì˜] {category} GT ë°ì´í„°ê°€ ë‹¨ì¼ í´ë˜ìŠ¤ì„ - AUC ê³„ì‚° ì œì™¸")
            AP1 = average_precision_score(category_gt, ap1)
            print(f"ğŸ“Š AP1: {AP1:.4f}")
            category_results[category] = {"AUC1": 0, "AP1": AP1, "AUC2": 0, "AP2": 0}
            continue
        
        ROC1 = roc_auc_score(category_gt, ap1)
        AP1 = average_precision_score(category_gt, ap1)
        ROC2 = roc_auc_score(category_gt, ap2)
        AP2 = average_precision_score(category_gt, ap2)

        print(f"ğŸ“Š AUC1: {ROC1:.4f}, AP1: {AP1:.4f}")
        print(f"ğŸ“Š AUC2: {ROC2:.4f}, AP2: {AP2:.4f}")

        category_results[category] = {"AUC1": ROC1, "AP1": AP1, "AUC2": ROC2, "AP2": AP2}

    print("\nğŸš€ ëª¨ë“  ì¹´í…Œê³ ë¦¬ í‰ê°€ ì™„ë£Œ\n")

    # ğŸ“Š ì„±ëŠ¥ ê·¸ë˜í”„ ì €ì¥
    plot_category_performance(category_results, gt_dir)

# **********************************************************************************************************
# ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ GT ë°ì´í„° ë¶„í¬ í™•ì¸
# **********************************************************************************************************

def analyze_category_gt_distribution(gt_dir, save_dir):
    """
    ğŸ“Œ Analyze and visualize frame-level distribution for each category.
    - Counts total frames, anomaly (1) frames, and normal (0) frames.
    - Saves distribution graph in the given directory.
    """

    # âœ… Load category-wise GT files
    category_gt_files = [f for f in os.listdir(gt_dir) if f.endswith("_GT.npy")]
    category_names = [f.replace("_GT.npy", "") for f in category_gt_files]

    # âœ… Dictionaries to store frame counts
    category_frame_counts = {}  # Total frames per category
    category_anomaly_counts = {}  # Anomaly (1) frames per category
    category_normal_counts = {}  # Normal (0) frames per category

    for category in category_names:
        gt_path = os.path.join(gt_dir, f"{category}_GT.npy")
        category_gt = np.load(gt_path)

        total_frames = len(category_gt)  # Total frames
        anomaly_frames = np.sum(category_gt == 1)  # Anomaly frames
        normal_frames = np.sum(category_gt == 0)  # Normal frames

        category_frame_counts[category] = total_frames
        category_anomaly_counts[category] = anomaly_frames
        category_normal_counts[category] = normal_frames

        print(f"ğŸ“Œ {category} - Total frames: {total_frames}, Anomaly frames: {anomaly_frames}, Normal frames: {normal_frames}")

    # âœ… Plot frame distribution (Bar Chart)
    categories = list(category_frame_counts.keys())
    total_values = [category_frame_counts[c] for c in categories]
    anomaly_values = [category_anomaly_counts[c] for c in categories]
    normal_values = [category_normal_counts[c] for c in categories]

    x = np.arange(len(categories))

    plt.figure(figsize=(12, 6))
    plt.bar(x - 0.2, total_values, width=0.3, label="Total Frames", color="blue")
    plt.bar(x, anomaly_values, width=0.3, label="Anomaly Frames (1)", color="red")
    plt.bar(x + 0.2, normal_values, width=0.3, label="Normal Frames (0)", color="green")

    plt.xticks(x, categories, rotation=45, ha="right")
    plt.ylabel("Frame Count")
    plt.title("Frame Distribution per Category")
    plt.legend()
    plt.grid(axis="y", linestyle="--", alpha=0.7)

    # âœ… Save the graph
    save_path = os.path.join(save_dir, "category_GT_distribution.png")
    plt.savefig(save_path, bbox_inches="tight", dpi=300)
    print(f"\nğŸ“‚ Distribution graph saved: {save_path}")

    plt.show()

# **********************************************************************************************************
# ğŸ“Š ì ìˆ˜ ê·¸ë˜í”„ ë¹„ë””ì˜¤ íŒŒì¼ ì €ì¥ & ì• ë‹ˆë©”ì´ì…˜ í•¨ìˆ˜ ìˆ˜í–‰
# **********************************************************************************************************

def clean_video_name(video_name):
    """
    ğŸ“Œ ë¹„ë””ì˜¤ ì´ë¦„ì—ì„œ '_x264' ì´í›„ì˜ ë‚´ìš©ì„ ì œê±°í•˜ì—¬ GTì™€ ì¼ì¹˜ì‹œí‚¤ëŠ” í•¨ìˆ˜
    ì˜ˆ: 'Abuse028_x264__5' -> 'Abuse028_x264'
    """
    match = re.match(r"(.+_x264)", video_name)
    return match.group(1) if match else video_name  # `_x264`ê¹Œì§€ë§Œ ìœ ì§€

def load_gt_from_annotation(annotation_path):
    """
    ğŸ“Œ Temporal_Anomaly_Annotation.txtì—ì„œ GT ì •ë³´ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    """
    gt_dict = {}
    with open(annotation_path, "r") as f:
        for line in f:
            parts = line.strip().split()
            video_name = parts[0]  # ì²« ë²ˆì§¸ ì—´ì´ ë¹„ë””ì˜¤ íŒŒì¼ëª…
            anomaly_frames = list(map(int, parts[2:]))  # ì‹œì‘~ë í”„ë ˆì„ ì •ë³´

            if video_name not in gt_dict:
                gt_dict[video_name] = []

            # GTì— ì •ìƒì ì¸ ì´ìƒ êµ¬ê°„ë§Œ ì¶”ê°€
            for i in range(0, len(anomaly_frames), 2):
                if anomaly_frames[i] != -1:
                    gt_dict[video_name].append((anomaly_frames[i], anomaly_frames[i+1]))

    return gt_dict

def real_time_scoring_animation_with_gt(model, input_dataloader, maxlen, prompt_text, device, output_root, annotation_path, fps=30):
    """
    ğŸ“Œ ì‹¤ì‹œê°„ Scoring + GT ë¹„êµ ì• ë‹ˆë©”ì´ì…˜ ìƒì„± (Frame-level GT ë°˜ì˜)
    - GTëŠ” Temporal_Anomaly_Annotation.txtì—ì„œ ë¡œë“œí•˜ì—¬ ì‚¬ìš©
    """

    model.to(device)
    model.eval()
    video_root_folder = "D:/UCF_Crimes/Videos"

    # ğŸ”¥ GT ë°ì´í„° ë¡œë“œ (ì´ë¦„ ì •ë¦¬ í›„ ë§¤ì¹­)
    gt_dict = load_gt_from_annotation(annotation_path)

    with torch.no_grad():
        for i, item in enumerate(input_dataloader):
            visual = item[0].squeeze(0)
            length = item[2].item()
            file_name = item[3][0]
            base_name = os.path.splitext(file_name)[0]
            clean_base_name = clean_video_name(base_name)  # ğŸ›  ì´ë¦„ ì •ë¦¬

            print(f"\nğŸ“Œ Processing: {base_name} -> {clean_base_name}")
            print(f"ğŸ“Š visual.shape: {visual.shape}, length: {length}")

            # ì›ë³¸ ë¹„ë””ì˜¤ ì°¾ê¸° & ì¹´í…Œê³ ë¦¬ëª… ì¶”ì¶œ
            video_path, category = get_original_video_path(video_root_folder, clean_base_name)
            if video_path is None:
                print(f"âŒ Error: No matching video found for {clean_base_name} in {video_root_folder}")
                continue  # ë¹„ë””ì˜¤ê°€ ì—†ìœ¼ë©´ ë„˜ì–´ê°

            # ğŸ“Œ GT ë°ì´í„° ë¡œë“œ (frame ë‹¨ìœ„)
            if clean_base_name not in gt_dict:
                print(f"âš  GT ì •ë³´ ì—†ìŒ: {clean_base_name}")
                continue
            anomaly_regions = gt_dict[clean_base_name]
            print(f"âœ… GT Loaded: {clean_base_name} -> {anomaly_regions}")

            # ë¹„ë””ì˜¤ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"âŒ Unable to open video file: {video_path}")

            video_fps = int(cap.get(cv2.CAP_PROP_FPS))
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.release()

            print(f"ğŸ¬ ì›ë³¸ ë¹„ë””ì˜¤ ì •ë³´: FPS = {video_fps}, ì´ í”„ë ˆì„ = {total_frames}")

            # ì¹´í…Œê³ ë¦¬ë³„ í´ë” ìƒì„± (ì—†ìœ¼ë©´ ìë™ ìƒì„±)
            category_folder = os.path.join(output_root, category)
            os.makedirs(category_folder, exist_ok=True)

            # Feature Chunks ìƒì„±
            num_chunks = int(np.ceil(length / maxlen))
            features_chunks = []
            for j in range(num_chunks):
                start_idx = j * maxlen
                end_idx = min((j + 1) * maxlen, length)
                chunk = visual[start_idx:end_idx]

                if chunk.shape[0] < maxlen:
                    padding = torch.zeros((maxlen - chunk.shape[0], visual.shape[1]), device=device)
                    chunk = torch.cat([chunk, padding], dim=0)

                features_chunks.append(chunk)

            print(f"ğŸ“Š Total Segments in npy: {length // 16}, Expected Chunks: {num_chunks}")

            if len(features_chunks) == 0:
                print(f"âš  Warning: No features_chunks created for {base_name}. Skipping...")
                continue

            # ğŸŸ¢ Model Inference & Score Calculation
            scores = []
            for chunk in features_chunks:
                chunk_tensor = chunk.unsqueeze(0).to(device)
                padding_mask = get_batch_mask(torch.tensor([maxlen]), maxlen).to(device)

                _, logits1, _ = model(chunk_tensor, padding_mask, prompt_text, lengths=[maxlen])
                prob1 = torch.sigmoid(logits1.view(-1)).detach().cpu().numpy()
                scores.extend(prob1)

            print(f"ğŸ“Š Scores computed: {len(scores)}")

            if len(scores) == 0:
                print(f"âš  Warning: No scores generated for {base_name}. Skipping...")
                continue

            # ğŸ¥ FPS ë§ì¶° Score Resampling
            frame_timestamps = np.linspace(0, total_frames / video_fps, total_frames)
            score_timestamps = np.linspace(0, total_frames / video_fps, len(scores))
            scores_resampled = np.interp(frame_timestamps, score_timestamps, scores)

            # âœ… GT í”„ë ˆì„ ë²¡í„° ìƒì„± (0: ì •ìƒ, 1: ì´ìƒ)
            gt_frame_vector = np.zeros(total_frames)
            for start, end in anomaly_regions:
                gt_frame_vector[start:end] = 1  # ì´ìƒ í–‰ë™ í”„ë ˆì„ ì„¤ì •

            # ğŸ¬ ì• ë‹ˆë©”ì´ì…˜ ì„¤ì •
            fig, ax = plt.subplots(figsize=(10, 3))
            ax.set_xlim(0, total_frames / video_fps)
            ax.set_ylim(0, 1)
            ax.set_xlabel("Time (seconds)")
            ax.set_ylabel("Score")
            ax.grid()

            line_score, = ax.plot([], [], label="Anomaly Score", color="red")
            line_gt, = ax.plot([], [], label="GT", color="blue", linestyle="dashed")
            scatter = ax.scatter([], [], color="black", s=50)
            ax.legend(loc="upper right")

            def init():
                line_score.set_data([], [])
                line_gt.set_data([], [])
                scatter.set_offsets(np.array([[], []]).T)
                return line_score, line_gt, scatter

            def update(frame_idx):
                x_data = frame_timestamps[:frame_idx + 1]
                y_score_data = scores_resampled[:frame_idx + 1]
                y_gt_data = gt_frame_vector[:frame_idx + 1]

                line_score.set_data(x_data, y_score_data)
                line_gt.set_data(x_data, y_gt_data)
                scatter.set_offsets(np.column_stack([x_data[-1:], y_score_data[-1:]]))

                return line_score, line_gt, scatter

            ani = FuncAnimation(fig, update, frames=len(frame_timestamps), init_func=init, blit=True)

            # ğŸ¥ ì¹´í…Œê³ ë¦¬ë³„ ì €ì¥ ê²½ë¡œ ì„¤ì •
            output_path = os.path.join(category_folder, f"{base_name}_scores_with_GT.mp4")
            writer = FFMpegWriter(fps=fps, metadata=dict(artist='RealTimeScoring'), bitrate=1800)
            ani.save(output_path, writer=writer)
            print(f"âœ… Animation saved to {output_path}")

            plt.close(fig)

# **********************************************************************************************************
# ğŸ“Œ ë¹„ë””ì˜¤ ì˜ìƒ ì…ë ¥ -> CLIP Feature Extraction
# **********************************************************************************************************

def extract_clip_features(video_path, clip_model, processor, device):
    """
    ë¹„ë””ì˜¤ì˜ ê° í”„ë ˆì„ì—ì„œ CLIP íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜.
    
    Args:
        video_path (str): ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        clip_model (CLIPModel): CLIP ëª¨ë¸
        processor (CLIPProcessor): CLIP ì…ë ¥ ì „ì²˜ë¦¬ê¸°
        device (str): ì‹¤í–‰ ì¥ì¹˜ (CPU/GPU)

    Returns:
        np.ndarray: CLIP íŠ¹ì§• ë²¡í„° ë¦¬ìŠ¤íŠ¸ (í”„ë ˆì„ ë‹¨ìœ„)
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"âŒ Unable to open video file: {video_path}")

    features = []
    print(f"ğŸ“Š Extracting CLIP features for video: {video_path}...")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # ğŸ“ í”„ë ˆì„ì„ CLIP ì…ë ¥ í¬ê¸°ë¡œ ì¡°ì • ë° ì „ì²˜ë¦¬
        frame_resized = cv2.resize(frame, (224, 224))  
        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)

        inputs = processor(images=frame_rgb, return_tensors="pt").to(device)
        with torch.no_grad():
            feature = clip_model.get_image_features(**inputs).cpu().numpy()
        features.append(feature)

    cap.release()
    features = np.vstack(features)  # ì „ì²´ íŠ¹ì§• ë²¡í„° ìŠ¤íƒ
    print(f"âœ… Extracted features shape: {features.shape}")
    return features

# **********************************************************************************************************
# ğŸ“Œ ë¹„ë””ì˜¤ ì…ë ¥ -> CLIP íŠ¹ì§• ì¶”ì¶œ -> ì´ìƒì¹˜ ì ìˆ˜ ê³„ì‚° -> anomaly_video_score_plotting í•¨ìˆ˜ í˜¸ì¶œ
# **********************************************************************************************************

def test_real_time_vadclip(model, video_path, visual_length, prompt_text, device):
    """
    ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ë¶„ì„ì„ ìˆ˜í–‰í•˜ì—¬ ì´ìƒ í–‰ë™ íƒì§€ë¥¼ ì§„í–‰í•˜ëŠ” í•¨ìˆ˜.

    Args:
        model (CLIPVAD): ì´ìƒ í–‰ë™ íƒì§€ ëª¨ë¸
        video_path (str): ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        visual_length (int): ëª¨ë¸ ì…ë ¥ì— ì‚¬ìš©í•  ì‹œí€€ìŠ¤ ê¸¸ì´
        prompt_text (str): CLIP í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
        device (str): ì‹¤í–‰ ì¥ì¹˜ (CPU/GPU)

    Returns:
        None
    """
    model.eval()
    model.to(device)

    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"âŒ Unable to open video file: {video_path}")

    features = []
    print(f"ğŸ“Š Extracting CLIP features for video: {video_path}...")

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # ğŸ“ í”„ë ˆì„ì„ CLIP ì…ë ¥ í¬ê¸°ë¡œ ë³€í™˜
        resized_frame = cv2.resize(frame, (224, 224)) / 255.0
        resized_frame = np.transpose(resized_frame, (2, 0, 1))  # HWC -> CHW ë³€í™˜
        resized_frame = torch.tensor(resized_frame, dtype=torch.float32).unsqueeze(0).to(device)

        # ğŸ¯ CLIP íŠ¹ì§• ì¶”ì¶œ
        with torch.no_grad():
            clip_features = model.clipmodel.encode_image(resized_frame).cpu().numpy()
        features.append(clip_features)

    cap.release()
    features = np.vstack(features)
    print(f"âœ… Extracted features shape: {features.shape}")

    # ğŸ“Œ íŠ¹ì§•ì„ visual_length ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì…ë ¥
    num_chunks = int(np.ceil(features.shape[0] / visual_length))
    features_chunks = []

    for i in range(num_chunks):
        start_idx = i * visual_length
        end_idx = min((i + 1) * visual_length, features.shape[0])
        chunk = features[start_idx:end_idx]

        # ğŸ³ï¸ ë¶€ì¡±í•œ ë¶€ë¶„ì€ 0ìœ¼ë¡œ íŒ¨ë”©
        if chunk.shape[0] < visual_length:
            padding = np.zeros((visual_length - chunk.shape[0], features.shape[1]), dtype=np.float32)
            chunk = np.vstack([chunk, padding])

        features_chunks.append(chunk)

    # ğŸ” ëª¨ë¸ì„ í†µí•´ ì´ìƒ í–‰ë™ ì ìˆ˜ ê³„ì‚°
    scores = []
    for chunk in features_chunks:
        chunk_tensor = torch.tensor(chunk, dtype=torch.float32).unsqueeze(0).to(device)
        padding_mask = get_batch_mask(torch.tensor([visual_length]), visual_length).to(device)

        _, logits1, _ = model(chunk_tensor, padding_mask, prompt_text, lengths=[visual_length])
        prob1 = torch.sigmoid(logits1.view(-1)).detach().cpu().numpy()

        scores.extend(prob1)

    # ğŸ“Š ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ë””ì˜¤ ì‹œê°í™” ìˆ˜í–‰
    visualize_video_anomaly_scores(
        video_path=video_path,
        scores=scores,
        output_path="anomaly_scores.mp4"
    )


# **********************************************************************************************************
# ğŸ“Š ìŠ¤ì½”ì–´ë¥¼ OpenCV ê·¸ë˜í”„ë¡œ í”Œë¡œíŒ…í•˜ì—¬ ì´ìƒ í–‰ë™ ì‹œê°í™”
# **********************************************************************************************************

def visualize_video_anomaly_scores(video_path, scores, output_path):
    """
    ì´ìƒ í–‰ë™ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¹„ë””ì˜¤ë¥¼ ì‹œê°í™”í•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜.

    Args:
        video_path (str): ì…ë ¥ ë¹„ë””ì˜¤ ê²½ë¡œ
        scores (list): ì´ìƒ í–‰ë™ ì ìˆ˜ ë¦¬ìŠ¤íŠ¸
        output_path (str): ì €ì¥í•  ë¹„ë””ì˜¤ íŒŒì¼ ê²½ë¡œ

    Returns:
        None
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"âŒ Unable to open video file: {video_path}")

    # ğŸ¥ VideoWriter ì„¤ì •
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height + 200))  

    frame_idx = 0
    fig, ax = plt.subplots(figsize=(10, 2))  

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret or frame_idx >= len(scores):
            break

        # ğŸ”´ ì ìˆ˜ì— ë”°ë¼ í”„ë ˆì„ì— ì´ìƒ íƒì§€ ê²°ê³¼ í‘œì‹œ
        anomaly_score = scores[frame_idx]
        frame = cv2.putText(
            frame,
            f"Anomaly Score: {anomaly_score:.4f}",
            (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX,
            0.8,
            (0, 0, 255) if anomaly_score > 0.5 else (0, 255, 0),  # ì´ìƒ ì ìˆ˜ 0.5 ì´ìƒì¼ ê²½ìš° ë¹¨ê°„ìƒ‰
            2
        )

        # ğŸ“Š ì ìˆ˜ ê·¸ë˜í”„ í”Œë¡œíŒ…
        ax.clear()
        ax.plot(range(frame_idx + 1), scores[:frame_idx + 1], label="Anomaly Score", color="red")
        ax.set_xlim(0, len(scores))
        ax.set_ylim(0, 1)
        ax.set_xlabel("Frame Index")
        ax.set_ylabel("Score")
        ax.legend(loc="upper right")
        ax.grid()

        # ğŸ“· ê·¸ë˜í”„ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        fig.canvas.draw()
        graph_img = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
        graph_img = graph_img.reshape(fig.canvas.get_width_height()[::-1] + (3,))
        graph_img = cv2.cvtColor(graph_img, cv2.COLOR_RGB2BGR)

        # ğŸ¬ í”„ë ˆì„ê³¼ ê·¸ë˜í”„ë¥¼ í•©ì„±
        combined_frame = np.zeros((height + 200, width, 3), dtype=np.uint8)
        combined_frame[:height, :, :] = frame
        graph_resized = cv2.resize(graph_img, (width, 200))
        combined_frame[height:, :, :] = graph_resized

        # ğŸ¥ ë¹„ë””ì˜¤ ì €ì¥
        out.write(combined_frame)

        # ğŸ” ì‹¤ì‹œê°„ í‘œì‹œ
        cv2.imshow("Anomaly Detection", combined_frame)
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

        frame_idx += 1

    cap.release()
    out.release()
    cv2.destroyAllWindows()
    print(f"âœ… Video with anomaly scores saved to {output_path}")

# **********************************************************************************************************
# ğŸ“Œ ë©”ì¸ ì‹¤í–‰
# **********************************************************************************************************

if __name__ == '__main__':
    # ğŸ”¥ ì¥ì¹˜ ì„¤ì • (CUDA ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPU í™œìš©)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    args = ucf_option.parser.parse_args()

    # ğŸ¯ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ (ë ˆì´ë¸” ë§¤í•‘ ì„¤ì •)
    label_map = dict({
        'Normal': 'Normal', 'Abuse': 'Abuse', 'Arrest': 'Arrest', 'Arson': 'Arson', 'Assault': 'Assault',
        'Burglary': 'Burglary', 'Explosion': 'Explosion', 'Fighting': 'Fighting', 'RoadAccidents': 'RoadAccidents',
        'Robbery': 'Robbery', 'Shooting': 'Shooting', 'Shoplifting': 'Shoplifting', 'Stealing': 'Stealing',
        'Vandalism': 'Vandalism'
    })

    # ğŸ“ í”„ë¡¬í”„íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
    prompt_text = get_prompt_text(label_map)

    # ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ë° ë¡œë“œ
    model = CLIPVAD(
        args.classes_num, args.embed_dim, args.visual_length, args.visual_width, args.visual_head,
        args.visual_layers, args.attn_window, args.prompt_prefix, args.prompt_postfix, device
    )
    model.load_state_dict(torch.load(args.model_path, map_location=device))

    # ğŸ–¼ï¸ CLIP ëª¨ë¸ ë° í”„ë¡œì„¸ì„œ ë¡œë“œ
    clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch16").to(device)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch16")

    # **********************************************************************************************************
    # ğŸ¯ UCF CRIME ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    if args.test_mode == 'test':
        print("ğŸš€ Running in test mode...")
        
        # ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ
        testdataset = UCFDataset(args.visual_length, args.test_list, True, label_map)  
        testdataloader = DataLoader(testdataset, batch_size=1, shuffle=False)  

        # ğŸ“Š Ground Truth ë°ì´í„° ë¡œë“œ
        gt = np.load(args.gt_path) 
        gtsegments = np.load(args.gt_segment_path, allow_pickle=True)
        gtlabels = np.load(args.gt_label_path, allow_pickle=True)

        # ğŸ” ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
        test(model, testdataloader, args.visual_length, prompt_text, gt, gtsegments, gtlabels, device)
    
    # **********************************************************************************************************
    # ğŸ¯ UCF CRIME ì¹´í…Œê³ ë¦¬ë³„ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
    if args.test_mode == 'Category_test':
        print("ğŸš€ Running in Category test mode...")
        
        # ğŸ” ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
        test_category_wise(model, args.category_gt_dir, args.visual_length, prompt_text, label_map, device)
    
    # **********************************************************************************************************
    # ğŸ“ˆ UCF CRIME ì¹´í…Œê³ ë¦¬ë³„ ë°ì´í„° ë¶„í¬ í™•ì¸
    if args.test_mode == 'gt_distribution':
        print("ğŸš€ Running in analyze category gt distribution mode...")
        
        # ğŸ” ëª¨ë¸ í‰ê°€ ìˆ˜í–‰
        analyze_category_gt_distribution(args.category_gt_dir, args.category_gt_dir)

    # **********************************************************************************************************
    # ğŸ“ˆ ì´ìƒì¹˜ ì ìˆ˜ ê·¸ë˜í”„ ìƒì„± ëª¨ë“œ
    if args.test_mode == 'plot_score':
        print("ğŸ“Š Running to plot score...")

        # ğŸ“‚ ë°ì´í„°ì…‹ ë¡œë“œ
        input_dataset = UCFDataset(args.visual_length, args.test_list, False, label_map)
        input_dataloader = DataLoader(input_dataset, batch_size=1, shuffle=False)

        # ğŸ¬ ì‹¤ì‹œê°„ ì´ìƒì¹˜ ì ìˆ˜ ì• ë‹ˆë©”ì´ì…˜ ìƒì„±
        real_time_scoring_animation_with_gt(
            model=model,
            input_dataloader=input_dataloader,
            maxlen=args.visual_length,
            prompt_text=prompt_text,
            device=device,
            output_root=args.score_video_save_path,
            annotation_path=args.annotation_path,
            fps=30
        )

    # **********************************************************************************************************
    # ğŸ¥ ì‹¤ì‹œê°„ ë¹„ë””ì˜¤ ì´ìƒ í–‰ë™ íƒì§€ ëª¨ë“œ
    if args.test_mode == 'test_real_time':
        print("ğŸ¥ Running in test_real_time mode...")

        # ğŸ” ì‹¤ì‹œê°„ VAD (Video Anomaly Detection) ì‹¤í–‰
        test_real_time_vadclip(
            model=model,
            video_path=args.video_path,
            prompt_text=prompt_text,
            device=device,
            visual_length=args.visual_length
        )
    
